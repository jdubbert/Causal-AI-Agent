# ðŸ§ª Causal Inference Agent

An AI-powered agent that **automatically selects, runs, and interprets causal inference methods** on your data. Give it a CSV and a business question â€” it profiles your data, picks the right method, validates assumptions with diagnostic checks, and generates a full markdown report.

Built with [Claude](https://docs.anthropic.com/en/docs/intro-to-claude) as the reasoning backbone and pure Python statistical implementations.

---

## How It Works

```
CSV + Business Question
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Data        â”‚  Detects columns: treatment, outcome,
  â”‚  Profiler    â”‚  time, geo â€” suggests methods
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Claude      â”‚  Picks the best method based on
  â”‚  (Reasoning) â”‚  data structure & question
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Analysis    â”‚  Runs the chosen method with
  â”‚  Engine      â”‚  full diagnostic checks
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  Diagnostics fail?
    â”‚         â”‚  â†’ Falls back to alternative method
    â–¼         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Report     â”‚  Structured markdown with estimates,
  â”‚  Generator  â”‚  confidence intervals, & recommendations
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Supported Methods

| Method | When It's Used | Key Diagnostics |
|--------|---------------|-----------------|
| **A/B Test** (Welch's t-test) | Cross-sectional treatment/control data | Sample Ratio Mismatch, Cohen's d, Normality check |
| **Difference-in-Differences** | Panel data with treatment groups + time periods | Parallel trends test, Placebo test, Pre-period balance |
| **Synthetic Control** | Geo/unit-level panel data; fallback when DiD fails | Pre-period fit (RMSPE), Post/Pre RMSPE ratio, Leave-one-out sensitivity |

The agent automatically falls back â€” for example, if DiD parallel trends fail, it tries Synthetic Control and reports both.

---

## Quick Start

### 1. Clone & install

```bash
git clone https://github.com/jdubbert/causal-inference-agent.git
cd causal-inference-agent
pip install -r requirements.txt
```

### 2. Set your API key

```bash
export ANTHROPIC_API_KEY="your-key-here"
```

### 3. Run the demo

```bash
python run_agent.py --demo
```

This runs all three methods on included synthetic datasets and saves reports to `causal_agent/data/`.

---

## Usage

### Analyze your own data

```bash
python run_agent.py \
  --data path/to/your_data.csv \
  --question "Did the new feature cause an increase in conversions?"
```

### All options

```
usage: run_agent.py [-h] [--data DATA] [--question QUESTION]
                    [--output OUTPUT] [--demo] [--chat] [--quiet]

options:
  --data      Path to CSV data file
  --question  Business question to answer
  --output    Output path for report (.md)
  --demo      Run all demo scenarios
  --chat      Interactive chat mode â€” ask follow-up questions
  --quiet     Suppress verbose output
```

### Interactive chat mode

Instead of a one-shot report, start a conversation with the agent about your data:

```bash
python run_agent.py --data causal_agent/data/ab_test.csv --chat
```

The agent runs the initial analysis, then drops you into a chat loop where you can ask follow-ups:

```
You: Was the sample split evenly?
ðŸ¤– Yes â€” 2,003 control vs 1,997 treatment. The SRM check passed (p=0.91),
   so the randomization looks clean.

You: What if I only look at customers with tenure > 12 months?
ðŸ¤– I'd need to re-run the analysis on a filtered subset. The current data
   shows tenure ranges from 1-240 months...

You: Is the effect size large enough to matter?
ðŸ¤– Cohen's d = 0.17, which is a small effect. The lift is statistically
   significant but practically modest at ~3.5 percentage points.

You: Can you explain the results to a non-technical stakeholder?
**What we tested:** We split ~4,000 customers into two equal groups â€” one got the treatment (the new experience/feature/offer) and the other didn't.

**What we found:** Customers who received the treatment were **more likely to activate**. About **15.5 out of 100** treated customers activated, compared to only **11.7 out of 100** in the group that didn't get it. That's roughly **1 in 3 more activations** than we'd normally expect.

**Can we trust this?** Yes. The groups were evenly split, and the result is very unlikely to be due to random chance. We're confident the true improvement is somewhere between **1.7 and 6 extra activations per 100 customers**.

**Bottom line:** The treatment works. It's not a massive effect per individual customer, but at scale across thousands of customers, those extra activations add up. I'd recommend rolling it out more broadly.

You: report
ðŸ“„ Report saved to: causal_agent/data/ab_test_report.md

You: quit
ðŸ‘‹ Goodbye!
```

You can also provide an initial question with chat mode:

```bash
python run_agent.py --data causal_agent/data/did.csv --chat --question "Did the campaign work?"
```

---

## Demo Datasets

Three synthetic datasets are included in `causal_agent/data/` (generated by `generate_demo_data.py`):

### A/B Test â€” Email Campaign â†’ Feature Activation
- **File**: `causal_agent/data/ab_test.csv`
- **Setup**: 4,000 customers, 50/50 random split
- **True effect**: ~3.5 pp lift in activation rate (12% â†’ 15.5%)
- **Columns**: `customer_id`, `treatment`, `activated`, `age`, `tenure_months`

```bash
python run_agent.py \
  --data causal_agent/data/ab_test.csv \
  --question "Did the email campaign increase checking account feature activations?"
```

### Difference-in-Differences â€” Marketing Campaign â†’ Revenue
- **File**: `causal_agent/data/did.csv`
- **Setup**: 20 markets (6 treatment, 14 control) Ã— 20 weeks
- **True effect**: ~8% revenue lift post-campaign
- **Columns**: `market`, `week`, `treatment`, `post`, `revenue`

```bash
python run_agent.py \
  --data causal_agent/data/did.csv \
  --question "Did the marketing campaign cause an incremental lift in revenue?"
```

### Synthetic Control â€” Geo Campaign â†’ DMA Conversions
- **File**: `causal_agent/data/synthetic_control.csv`
- **Setup**: 15 DMAs Ã— 52 weeks, 1 treated DMA
- **True effect**: ~12% lift starting week 27
- **Columns**: `dma`, `week`, `conversions`, `treated`, `post`

```bash
python run_agent.py \
  --data causal_agent/data/synthetic_control.csv \
  --question "What was the causal impact of the campaign on the treated DMA?"
```

---

## Sample Output

Here's a snippet from the A/B test report:

```
### Primary Analysis: A/B Test (Two-Sample T-Test)

| Metric       | Value                            |
|------------- |----------------------------------|
| ATT Estimate | +0.0350                          |
| Relative Lift| +29.17%                          |
| 95% CI       | [0.0210, 0.0490]                 |
| P-value      | 0.0000                           |
| Significance | highly significant (p < 0.001)   |

Diagnostics:
  âœ… Sample Ratio Mismatch â€” No SRM detected
  âœ… Cohen's d = 0.17 (small effect size)

Finding: Statistically significant causal effect with all diagnostics passed.
```

Full sample reports are in the repo: `sample_report_ab_test.md`, `sample_report_did.md`, `sample_report_sc.md`.

---

## Project Structure

```
â”œâ”€â”€ run_agent.py                  # CLI entrypoint
â”œâ”€â”€ generate_demo_data.py         # Creates the 3 synthetic datasets
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ causal_agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py                 # Pydantic models (AgentState, DataProfile, MethodResult)
â”‚   â”œâ”€â”€ runner.py                 # Agent loop â€” Claude tool-use orchestration
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ profiler.py           # Auto-detects columns & suggests methods
â”‚       â”œâ”€â”€ ab_test.py            # Welch's t-test + SRM + Cohen's d
â”‚       â”œâ”€â”€ did.py                # Two-way FE DiD + parallel trends + placebo
â”‚       â”œâ”€â”€ synthetic_control.py  # Abadie-Diamond-Hainmueller SC method
â”‚       â””â”€â”€ reporter.py           # Markdown report generator
â””â”€â”€ sample_report_*.md            # Example output reports
```

---

## Data Format Requirements

Your CSV should include some combination of these columns (the profiler auto-detects them):

| Column Type | Example Names | Required For |
|-------------|--------------|-------------|
| **Treatment** | `treatment`, `treated`, `group`, `variant` | All methods |
| **Outcome** | `revenue`, `conversions`, `activated`, `sales` | All methods |
| **Time** | `week`, `date`, `month`, `period` | DiD, Synthetic Control |
| **Geo/Unit** | `dma`, `market`, `state`, `region` | Synthetic Control |
| **Post indicator** | `post`, `is_post`, `after` | DiD (optional â€” auto-splits if missing) |

---

## How the Agent Reasons

The agent uses Claude's tool-use capability to run an iterative analysis loop:

1. **Profile** â†’ auto-detect data structure
2. **Select method** â†’ Claude picks the best fit based on structure + question
3. **Run analysis** â†’ execute the statistical method
4. **Check diagnostics** â†’ if assumptions fail, adapt (e.g., DiD â†’ Synthetic Control)
5. **Generate report** â†’ structured markdown with estimates, diagnostics, and interpretation

All reasoning steps are logged and included in the final report.

---

## Requirements

- Python 3.10+
- An [Anthropic API key](https://console.anthropic.com/)
- Dependencies: see `requirements.txt`

---

## License

MIT License â€” see [LICENSE](LICENSE).
